{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "sent_twit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VipanchiKatthula/TwitterSentimentAnalysis/blob/master/sent_twit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl9xKnEReLoJ",
        "colab_type": "code",
        "outputId": "be83ff21-8dda-46fc-9d8b-993f1338f430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import coo_matrix, hstack\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KUcbd9_fQ6g",
        "colab_type": "code",
        "outputId": "3e801687-a22e-46f3-c0b5-8dbce935e5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip install -q xlrd\n",
        "!git clone https://github.com/VipanchiKatthula/TwitterSentimentAnalysis.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TwitterSentimentAnalysis'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsqrTW9qgcJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linelist1=[]\n",
        "linelist2=[]\n",
        "target1 = []\n",
        "target2 = []\n",
        "with open ('TwitterSentimentAnalysis/train_90.txt',encoding=\"utf8\") as t1:\n",
        "    for line in t1:\n",
        "        linelist1.append(word_tokenize((line.split(',',4)[3].lstrip())))\n",
        "        target1.append(line.split(',', 4)[1])\n",
        "with open ('TwitterSentimentAnalysis/test_10.txt',encoding=\"utf8\") as t2:\n",
        "    for line in t2:\n",
        "        linelist2.append(word_tokenize((line.split(',',4)[3].lstrip())))\n",
        "        target2.append(line.split(',', 4)[1])        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx0r7oYPeLoh",
        "colab_type": "code",
        "outputId": "36311b0a-51e2-4d55-ef58-adbe89ce3ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(linelist1[1:4])\n",
        "print(linelist2[1:4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['is', 'so', 'sad', 'for', 'my', 'APL', 'friend', '...', '...', '...', '...', '.'], ['I', 'missed', 'the', 'New', 'Moon', 'trailer', '...'], ['omg', 'its', 'already', '7:30', ':', 'O']]\n",
            "[['``', '@', 'CleverMonkeys', 'but', 'AFTER', 'you', 'hit', 'up', 'the', 'Magnolia', 'to', 'see', 'ANVIL', '!', 'THE', 'STORY', 'OF', 'ANVIL', 'and', 'the', 'band', 'playing', 'a', 'small', 'set', 'shortly', 'after'], ['@', 'CleverMonkeys', 'only', 'seen', 'a', 'boot', 'on', 'a', 'car', 'in', 'movies', '!', 'goodthing', 'I', \"'m\", 'a', 'vegetarian', 'I', 'will', 'only', 'be', 'eatting', 'beans', 'until', 'the', 'end', 'of', 'month', '!'], ['@', 'clevertitania', 'Good', 'morning', '.', 'We', 'have', 'rain', 'and', 'thunder', 'here']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd5pzAqueLo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_tweets_train = []\n",
        "list_stop_words = set(stopwords.words('english'))\n",
        "for i in range (1,len(linelist1)):\n",
        "    token = linelist1[i]\n",
        "    filtoken=[]\n",
        "    for word in token:\n",
        "        if word not in list_stop_words:\n",
        "            if word.isalpha():\n",
        "                filtoken.append(sno.stem(word.lower()))\n",
        "    all_tweets_train.append(filtoken)\n",
        "\n",
        "all_tweets_test = []\n",
        "for i in range (1,len(linelist2)):\n",
        "    token = linelist2[i]\n",
        "    filtoken=[]\n",
        "    for word in token:\n",
        "        if word not in list_stop_words:\n",
        "            if word.isalpha():\n",
        "                filtoken.append(sno.stem(word.lower()))\n",
        "    all_tweets_test.append(filtoken)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HuIcRIMReLo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = all_tweets_train\n",
        "b = all_tweets_test\n",
        "lis1 = []\n",
        "lis2 = []\n",
        "def listToString(s):  \n",
        "    str1 = \" \"    \n",
        "    return (str1.join(s))              \n",
        "for ele in a:\n",
        "    strg = listToString(ele)\n",
        "    lis1.append(strg)\n",
        "for ele in b:\n",
        "    strg = listToString(ele)\n",
        "    lis2.append(strg)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgcmVOHbeLo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "train_vectors = pd.DataFrame( data = vectorizer.fit_transform(lis1).todense(),columns = vectorizer.get_feature_names())\n",
        "test_vectors = pd.DataFrame( data = vectorizer.transform(lis2).todense(),columns = vectorizer.get_feature_names())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue_-bWrcwa-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target1.pop(0)\n",
        "target2.pop(0)\n",
        "target1 = [float(i) for i in target1]\n",
        "target2 = [float(i) for i in target2]\n",
        "y_label = pd.DataFrame( data = target1)\n",
        "y_test = pd.DataFrame( data = target2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCS-CLZteLpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, alp=0.01, niter=1000):\n",
        "        self.lr = alp\n",
        "        self.niter = niter\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))   #test.values.astype(float)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        no_of_cols = 10000#X.shape[1]\n",
        "        self.theta = np.zeros((no_of_cols,1))\n",
        "        for i in range(self.niter):\n",
        "            z = np.dot(X, self.theta)\n",
        "            y_out = self.sigmoid(z)\n",
        "            y_out = y_out.reshape(y.shape)\n",
        "            gradient = np.dot(X.T, (y_out - y)) / y.size\n",
        "            self.theta -= self.lr * gradient\n",
        "            \n",
        "            if(i % 10000 == 0):\n",
        "                z = np.dot(X, self.theta)\n",
        "                y_out = self.sigmoid(z)\n",
        "\n",
        "    def predict_prob(self, X):\n",
        "        return self.sigmoid(np.dot(X, self.theta))\n",
        "    def predict(self, X, th):\n",
        "        label_array = np.array([])\n",
        "        pred = self.predict_prob(X)\n",
        "        \n",
        "        for i in pred:\n",
        "          if i > th:\n",
        "            label_array = np.append(label_array,1)\n",
        "          else:\n",
        "            label_array = np.append(label_array,0)\n",
        "        return label_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMpM91K9eLpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train_vectors.to_numpy()\n",
        "X_test = test_vectors.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVtP1oOIeLpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zxil2-u6jdD",
        "colab_type": "code",
        "outputId": "60552304-3706-45b8-8a85-a48d3d32ea01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "kf = KFold(n_splits=10)\n",
        "sc = np.array([])\n",
        "count = 1\n",
        "for train_index, test_index in kf.split(train_vectors):\n",
        "  x_train, x_test = train_vectors.iloc[train_index],train_vectors.iloc[test_index]\n",
        "  y_train, y_test = y_label.iloc[train_index],y_label.iloc[test_index]\n",
        "  logreg_model = LogisticRegression()\n",
        "  print(\"Runing model number:\",count)\n",
        "  logreg_model.fit(x_train.to_numpy(),y_train.to_numpy())\n",
        "  pred = logreg_model.predict_prob(x_test.to_numpy())\n",
        "  th = sum(pred)/len(pred)\n",
        "  labels = logreg_model.predict(x_test,th)\n",
        "  print(\"completed!! model number:\",count)\n",
        "  sc = np.append(sc,accuracy_score(y_test,labels))\n",
        "  count +=1\n",
        "print(sc)\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runing model number: 1\n",
            "completed!! model number: 1\n",
            "Runing model number: 2\n",
            "completed!! model number: 2\n",
            "Runing model number: 3\n",
            "completed!! model number: 3\n",
            "[0.64986667 0.6228     0.61456667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEC1dkjK7LQ9",
        "colab_type": "code",
        "outputId": "dddbe0e9-d342-4f75-ea67-45768489466d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object _BaseKFold.split at 0x7fa336799bf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82q3iUWXiNqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_out = model.predict_prob(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KYMzEHZ0qyD",
        "colab_type": "code",
        "outputId": "bf02b05d-5da8-4891-844c-3c2bcff5e99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(test_out)/len(test_out>0.53)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.50130522])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSjnv0bf2Z7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "rcount =0\n",
        "for i in test_out:\n",
        "  if i< 0.50130522:\n",
        "    count += 1\n",
        "  else: rcount +=1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydepl96-3BR8",
        "colab_type": "code",
        "outputId": "14f1a765-275d-469f-d2c1-9235d16c00e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(count,rcount)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6552 3448\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
